\documentclass{article}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{color}
\usepackage{subfig}
\usepackage{physics}
\graphicspath{ {figures/} }

\title{Monte Carlo Methods}
\author{Caitlin Carnahan}
\begin{document}
This document is intended to serve as personal notes
for self-study of \emph{Monte Carlo Methods for Statistical Physics}
by M.E.J. Newman and G.T. Barkema.

\section{Introduction}
\subsection{Statistical Mechanics}
Many systems of interest are composed of very many simple
constituent systems. For example, a liter of oxygen
is composed of $~10^{22}$ oxygen molecules. The equations
of motion for a single oxygen molecule are relatively simple
but the sheer number of oxygen molecules in the larger system
render the exact solutions infeasible. Instead, we approach
the larger system via probabilistic methods. The goal is to
express the state of the larger system as a set of probabilities
of being in one state or another.

The typical systems studied in physics for which MCM are useful
are systems described by a Hamiltonian $H$ and an
associated energy spectrum that is either continuous or given
by a discrete set $E_{0}$, $E_{1}$, $E_{2}$, etc. We will also
consider systems that interact with a thermal reservoir with
which the system can exchange heat. Heat exchanges with the
reservoir manifest as a small (negligible) perturbation
in the Hamiltonian which pushes the system from one energy
state to another.

Modeling the effect of the perturbation on the Hamiltonian is
done by defining a \emph{dynamics} for the system. The dynamics
of the system may take the form of a set of transition rates.
Suppose the system is in a state $\mu$. Then, $R(\mu \rightarrow \nu)dt$ is
defined to be the probability that the system transitions to a state
$\nu$ in time $dt$, where $R(\mu \rightarrow \nu)$ is defined to be the
\emph{transition rate}. We may also define a set of weights $w_{\mu}(t)$
which represent that probability that the system will be found in state
$\mu$ at time $t$. Using these elements, we can write down a \emph{master
equation}:
\begin{equation}
\frac{dw_{\mu}}{dt} = \sum_{\nu}[w_{\nu}(t)R(\nu \rightarrow \mu)
                              - w_{\mu}(t)R(\mu \rightarrow \nu)]
\end{equation}
The first term in the sum is the rate at which the system is
transitioning into state $\mu$. The second term is the rate at which
the system is transitioning out of state $\mu$.

We may define the expectation value of some observable $Q$, which
takes the value $Q_{\mu}$ in state $\mu$, as
$$ \left \langle Q \right \rangle = \sum_{\mu} Q_{\mu}w_{\mu}(t) $$
The expectation value of $Q$ can be interpreted two ways:
\begin{itemize}
\item It is the mean of the observed value of $Q$ that would be
obtained if we measured many identically prepared systems simultaneously.
\item It is the time average of the quantity $Q$. That is, it might be the
average observed value of $Q$ after many measurements over time.
\end{itemize}

\subsection{Equilibrium}
An \emph{equilibrium} state can be defined as a state in which all of the
rates of change $dw_{\mu}/dt$ vanish, and therefore the transition rate
into and out of each state $\mu$ must be equal. All systems goverened
by the master equation must come to equilibrium after some time and so
we can define the equilibrium values of the weights, the
\emph{equilibrium occupation probabilities}, as
$$p_{\mu} = \lim_{t\rightarrow \infty} w_{\mu}(t)$$
For a system in equilibrium with thermal reservoir at temperature T, the
Boltzmann distribution is
$$p_{\mu} = \frac{1}{Z}e^{-E_{\mu}/kT} = \frac{1}{Z}e^{-\beta E_{\mu}}$$
where $Z$ is the normalization constant, called the \emph{partition function}, and
defined as
$$Z = \sum_{\mu}e^{-\beta E_{\mu}}$$
Plugging this into our definition of expectation values gives us
$$ \left \langle Q \right \rangle = \sum_{\mu} Q_{\mu}p_{\mu}(t)
                                  = \frac{1}{Z}\sum_{\mu} Q_{\mu}e^{-\beta E_{\mu}} $$
which is the expression for the expectation value of a system in equilibrium.
For example, the internal energy $U$ of a system is given by
$$U = \left \langle E \right \rangle
                                  = \frac{1}{Z}\sum_{\mu} E_{\mu}e^{-\beta E_{\mu}}
                                  = -\frac{1}{Z}\frac{\partial Z}{\partial \beta}
                                  = -\frac{\partial log Z}{\partial \beta}$$
It is sometimes appropriate in MCM calculations to calculate the partition function, from which
many other quantities can be derived. Some examples can be found in Section 1.2 of Newman and
Barkema.

\subsection{Fluctuations, Correlations, and Responses}
It is useful to assess the fluctuations in observable quantities. Doing so
allows us to determine how the quantity we are measuring varies over time and
therefore how much of an approximation is being made by calculating the expectation
value. As an example, the root-mean-square fluctuation in internal energy is given by
$$\sqrt{\left \langle E^{2} \right \rangle - \left \langle E \right \rangle^{2}} = \sqrt{\frac{\partial^{2}log Z}{\partial \beta^{2}}}$$
In the limit of a large system, called the \emph{thermodynamic limit}, it is common for
fluctuations to become negligible. For example, RMS fluctuations in internal energy scale
as $\sqrt{V}$ in a system of volume $V$, but the internal energy itself scales as $V$
so as the system becomes very large, the fluctuations grow increasingly
negligible.

Every parameter that can be fixed in a system (e.g. volume, external field)
has a conjugate variable (e.g. pressure, magnetization) that is given by a
derivative of the free energy. For example,
$$ p = -\frac{\partial F}{\partial V}$$
$$ M = \frac{\partial F}{\partial B}$$
The Hamiltonian contains terms of the form $-XY$ where $Y$ is a "field" whose
value is fixed, and $X$ is the conjugate variable to which it couples.
A common trick to calculate the thermal average of a quantity is the following:
\begin{itemize}
\item Make up a fictitious field that couples to the quantity of interest.
\item Add an appropriate term to the Hamiltonian.
\item Set the field to zero after performing the derivative on the free energy.
\end{itemize}
The \emph{susceptibility} of $X$ to $Y$ is a measurement of the strength of
the response of $X$ to variation in $Y$. Typically, this quantity is denoted
by $\chi$ and can be defined as
$$\chi = \frac{\partial \left \langle X \right \rangle  }{\partial Y}$$
The \emph{linear response theorem} tell us that the fluctuations in a variable
are proportional to the susceptibility of the variable to its conjugate field.

\subsubsection{Correlation Functions}
The \emph{disconnected correlation function} $G^{(2)}(i,j)$ is defined as
$$G^{(2)}(i,j) = \left \langle x_{i}x_{j} \right \rangle$$
This function gives us roughly an idea of how the variables $x_{i}$ and $x_{j}$
are correlated. Consider the following scenarios:
\begin{itemize}
\item If $x_{i}$ and $x_{j}$ move (roughly) together most of the time around
the origin zero, then the average of their product will be a positive value.
\item If $x_{i}$ and $x_{j}$ move (roughly) together but in opposite dirrections
most of the time, then the average of their product will be a negative value.
\item If $x_{i}$ and $x_{j}$ move with no discernible pattern relative to one
another, then their product will be sometimes positive and sometimes negative,
averaging to zero over a long time.
\end{itemize}

This definition can be problematic in the case of a \emph{spontaneously
broken symmetry state}. This occurs when a variable $x$ develops a non-zero
expectation value. If this happens, $x_{i}$ and $x_{j}$ may, for instance,
always be large positive values leading to a large positive correlation value
even if they have nothing to do with one another. To address this problem, we
can use the \emph{two-point connected correlation function} $G_{c}^{(2)}(i,j)$
defined as
$$G_{c}^{(2)}(i,j) = \left \langle x_{i}x_{j} \right \rangle - \left \langle x_{i} \right \rangle \left \langle x_{j} \right \rangle$$
$$ = \left \langle (x_{i} - \left \langle x_{i} \right \rangle) \times (x_{j} - \left \langle x_{j} \right \rangle) \right \rangle $$
For reference, the higher-order connected correlation functions for three and four sites can be found in
section 1.2.1.

\subsection{Example: The Ising Model}
The two-dimensional Ising model of a magnet consists of a lattice of sites, where
each site contains a magnetic dipole or spin. Each spin is a two-level system
denoted by the variable $s_{i}$, which can only take on the values $\pm 1$. Exchange
interactions are modeled by including terms proportional to $s_{i}s_{j}$ in the
Hamiltonian. In the simplest case, these terms are identical for each pair of spins
(indicating the same interaction strength $J$ all over the lattice). Another simplification
is to consider only nearest neighbor interactions. An external magnetic field $B$
can also be introduced. In this case we obtain the following Hamiltonian
$$H = -J\sum_{\left \langle ij \right \rangle }s_{i}s_{j} - B\sum_{i}s_{i}$$
where $\left \langle ij \right \rangle$ indicates a sum over nearest neighbors. The minus
signs are conventional and fix the signs of the parameters $J$ and $B$. In the current form:
\begin{itemize}
\item A positive $J$ value (\emph{ferromagnetic}) makes the spins want to align to minimize the Hamiltonian.
\item A negative $J$ value (\emph{anti-ferromagnetic}) makes the spins want to anti-align.
\item A positive $B$ value makes the spins want to orient positively.
\item A negative $B$ value makes the spins want to orient negatively - that is, they
want to align with the magnetic field.
\end{itemize}
Each state of the Ising system is an specific "setting" of the spins in the lattice. For $N$
sites in the lattice, there are $2^{N}$ states. Earlier we defined the partition function
$Z$ as
$$Z = \sum_{\mu}e^{-\beta E_{\mu}}$$
where \{$\mu$\} represents a set of states. So our Ising partition function is given by
$$Z = \sum_{s_{1} = \pm 1} \sum_{s_{2} = \pm 1}...\sum_{s_{N}=\pm 1}e^{-\beta H} = \sum_{\{s_{i}\}} e^{-\beta H}$$
From the partition function, we can calculate the internal energy, entropy, free energy, specific heat, and so on (see section
1.2). The mean magnetization $\left \langle M \right \rangle $ is given by
$$\left \langle M \right \rangle  = \left \langle \sum_{i}s_{i} \right \rangle $$
and the mean magnetization per spin $\left \langle m \right \rangle$ is just
$$\left \langle m \right \rangle = \frac{1}{N} \left \langle M \right \rangle $$
The magnetic susceptibility is calculated as
$$\frac{\partial \left \langle M \right \rangle }{\partial B} = \beta (\left \langle M^{2} \right \rangle - \left \langle M \right \rangle^{2}  )$$
and the magnetic susceptibility per spin is calculated as
$$\chi = \frac{1}{N}\frac{\partial \left \langle M \right \rangle }{\partial B}$$
The specific heat is given by
$$c = \frac{k\beta^{2}}{N}(\left \langle E^{2} \right \rangle - \left \langle E \right \rangle^{2}  )$$

If we introduce a spatially-varying magnetic field by subscripting $B$, our
Hamiltonian becomes
$$H = -J\sum_{\left \langle ij \right \rangle }s_{i}s_{j} - \sum_{i}B_{i}s_{i}$$
and the mean magnetization per site becomes
$$\left \langle m_{i} \right \rangle = \left \langle s_{i} \right \rangle = \frac{1}{\beta}\frac{\partial \log Z}{\partial B_{i}}$$
which allows us to write down the connected correlation function
$$G_{c}^{(2)}(i,j) = \frac{1}{\beta^{2}}\frac{\partial^{2} \log Z}{\partial B_{i}\partial B_{j}}$$

\subsection{Numerical Methods}
There are several computational methods that can be used to solve
for quantities of interest in a given model. The most straightforward method
involves mapping the model of interest to a lattice of finite size so that
the partition function is exactly calculable by computer.

\subsubsection{Example: 2D Ising Model}
We start with a two-dimensional lattice of 25 spins, arranged in a 5$\times$5 grid.
We assume periodic boundary conditions such that spins at one end of the lattice
experience interations with spins on the other end. We also start with no externak
magnetic field ($B = 0$).

There are $2^{25}$ possible configurations, or states. However, we can utilize
the symmetry of the system to argue that every state has a reflective partner
where every spin is flipped. If $B = 0$, this state must have the same energy
as its reflective partner. Therefore, we only need to consider one of each pair in
the calculation of our partition function and then double the sum.

To calculate the mean magnetization per spin, we can use
$$\left \langle m \right \rangle = \frac{1}{N} \left \langle M \right \rangle = \frac{1}{N}\left \langle \sum_{i}s_{i} \right \rangle $$
and to calculate the specific heat per spin we use
$$C = \frac{\partial U}{\partial T} = -k\beta^{2}\frac{\partial U}{\partial \beta} = k\beta^{2}\frac{\partial^{2} \log Z}{\partial \beta^{2}}$$
To do: calculate partition function, specific heat and mean magnetization and plot last two. Compare against closed form equations by Onsager.
The key conclusion is that the physics is not well modeled on a finite lattice around the critical point ($kT = 2.3J$), but we stil get fairly
accurate results away from this point. It is still desirable to study the largest finite-sized system possible
to most accurately reason about the properties of the system in the thermodynamic limit. However, brute force calculations
scale rapidly in time with the lattice size. We must find other ways to calculate quantities of interest on a large lattice size.

\subsubsection{Monte Carlo Methods}
To calculate the partition function of a larger lattice for the Ising model, we turn to
Monte Carlo simulation. The fundamental idea is to simulate the thermal fluctuations from state
to state - in accordance with a set of weights $w_{\mu}(t)$ as they would be in the real system.
We then take a time average of the quantity of interest, which is regarded as an expectation value
of that quantity.

For this approach to work, we must find an accurate dynamics for the system that will govern its
time evoluation. That is, we must determine the master equation of the system. We do this by
\begin{itemize}
\item Choosing a set of rates $R(\mu \rightarrow \nu)$ for the transitions.
\item Forcing the equilibrium solution for the master equation to be the Boltzmann distribution.
\item Use this set of rates to time-evolve the simulated system.
\item Estimate the observable quantity in which we're interested.
\end{itemize}

This approach has the benefit that we do not need to recreate every state to get an accurate solution.
We only need to time-evolve through a small fraction of states. However, there are statistical errors
that arise from not recreating every state (these can be reduced by lengthening the simulation time).
For this reason, it is not wise to calculating the partition function and use this to calculate other
values. Instead, we should calculate quantities of interest directly (to reduce the noise of the result).

\subsection{Problems}
\begin{itemize}
\item \textbf{1.1} "If a system is in equilibrium with a thermal reservoir at temperature $T$, the probability
of its having a total energy $E$ varies with $E$ in proportion to $e^{-\beta E}.$" True of False?
{\color{red} False. If a system is in equilibrium with a thermal reservoir of temperature $T$, the
probability of its having a total energy $E$ is given by its probability of being in any state with characteristic
energy $E$. This can be written as
$$P_{E} = \frac{1}{Z}\sum_{E_{\mu} = E}e^{-\beta E_{\mu}}$$
The partition function factor $1/Z$ is found in all $P_{E}$ values for any given $E$. We can also note that,
by definition, all $E_{\mu}$ values in the sum of exponentials are simply $E$. Therefore, if we know the degeneracy
of the energy value $E$ (denoted $g_{E}$), then the probability of the system having total energy $E$ is given by
$$P_{E} = \frac{1}{Z}g_{E}e^{-\beta E} \propto g_{E}e^{-\beta E}$$
}
\item \textbf{1.2} A certain simple system has only two energy states, with energies $E_{0}$ and $E_{1}$, and
transitions between the two states take place at rates $R(0 \rightarrow 1) = R_{0}exp[-\beta (E_{1} - E_{0})]$
and $R(1 \rightarrow 0) = R_{0}$. Solve the master equation for the probabilities $w_{0}$ and $w_{1}$ of
occupation of the two states as a functions of time with the initial conditions $w_{0} = 0$, $w_{1} = 1$.
Show that as $t \rightarrow \infty$ these solutions tend to the Boltzmann probabilities.
\item \textbf{1.3} A slightly more complex system contains $N$ distinguishable particles, each of which can be in one of two
boxes. The particles in the first box have energy $E_{0} = 0$ and the particles in the second have energy $E_{1}$, and
particles are allowed to move back and forward between the boxes under the influence of thermal excitations from a
reservoir at temperature $T$. Find the partition function for this system and then use this result to calculate the internal
energy.
\item \textbf{1.4} Solve the Ising model in one dimension for the case $B = 0$ as follows. Define a new set
of variables $\sigma_{i}$ which take values $0$ and $1$ according to
$\sigma_{i} = \frac{1}{2}(1 - s_{i}s_{i+1})$ and rewrite the Hamiltonian in terms of these
variables for a system of $N$ spins with periodic boundary conditions. Show that the resulting system is
equivalent to the one studied in Problem 1.3 in the limit of large $N$ and hence calculate the internal
energy as a function of temperature.
\end{itemize}


\end{document}
