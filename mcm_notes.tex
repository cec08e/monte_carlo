\documentclass{article}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{physics}
\graphicspath{ {figures/} }

\title{Monte Carlo Methods}
\author{Caitlin Carnahan}
\begin{document}
This document is intended to serve as personal notes
for self-study of \emph{Monte Carlo Methods for Statistical Physics}
by M.E.J. Newman and G.T. Barkema.

\section{Introduction}
\subsection{Statistical Mechanics}
Many systems of interest are composed of very many simple
constituent systems. For example, a liter of oxygen
is composed of $~10^{22}$ oxygen molecules. The equations
of motion for a single oxygen molecule are relatively simple
but the sheer number of oxygen molecules in the larger system
render the exact solutions infeasible. Instead, we approach
the larger system via probabilistic methods. The goal is to
express the state of the larger system as a set of probabilities
of being in one state or another.

The typical systems studied in physics for which MCM are useful
are systems described by a Hamiltonian $H$ and an
associated energy spectrum that is either continuous or given
by a discrete set $E_{0}$, $E_{1}$, $E_{2}$, etc. We will also
consider systems that interact with a thermal reservoir with
which the system can exchange heat. Heat exchanges with the
reservoir manifest as a small (negligible) perturbation
in the Hamiltonian which pushes the system from one energy
state to another.

Modeling the effect of the perturbation on the Hamiltonian is
done by defining a \emph{dynamics} for the system. The dynamics
of the system may take the form of a set of transition rates.
Suppose the system is in a state $\mu$. Then, $R(\mu \rightarrow \nu)dt$ is
defined to be the probability that the system transitions to a state
$\nu$ in time $dt$, where $R(\mu \rightarrow \nu)$ is defined to be the
\emph{transition rate}. We may also define a set of weights $w_{\mu}(t)$
which represent that probability that the system will be found in state
$\mu$ at time $t$. Using these elements, we can write down a \emph{master
equation}:
\begin{equation}
\frac{dw_{\mu}}{dt} = \sum_{\nu}[w_{\nu}(t)R(\nu \rightarrow \mu)
                              - w_{\mu}(t)R(\mu \rightarrow \nu)]
\end{equation}
The first term in the sum is the rate at which the system is
transitioning into state $\mu$. The second term is the rate at which
the system is transitioning out of state $\mu$.

We may define the expectation value of some observable $Q$, which
takes the value $Q_{\mu}$ in state $\mu$, as
$$ \left \langle Q \right \rangle = \sum_{\mu} Q_{\mu}w_{\mu}(t) $$
The expectation value of $Q$ can be interpreted two ways:
\begin{itemize}
\item It is the mean of the observed value of $Q$ that would be
obtained if we measured many identically prepared systems simultaneously.
\item It is the time average of the quantity $Q$. That is, it might be the
average observed value of $Q$ after many measurements over time.
\end{itemize}

\subsection{Equilibrium}
An \emph{equilibrium} state can be defined as a state in which all of the
rates of change $dw_{\mu}/dt$ vanish, and therefore the transition rate
into and out of each state $\mu$ must be equal. All systems goverened
by the master equation must come to equilibrium after some time and so
we can define the equilibrium values of the weights, the
\emph{equilibrium occupation probabilities}, as
$$p_{\mu} = \lim_{t\rightarrow \infty} w_{\mu}(t)$$
For a system in equilibrium with thermal reservoir at temperature T, the
Boltzmann distribution is
$$p_{\mu} = \frac{1}{Z}e^{-E_{\mu}/kT} = \frac{1}{Z}e^{-\beta E_{\mu}}$$
where $Z$ is the normalization constant, called the \emph{partition function}, and
defined as
$$Z = \sum_{\mu}e^{-\beta E_{\mu}}$$
Plugging this into our definition of expectation values gives us
$$ \left \langle Q \right \rangle = \sum_{\mu} Q_{\mu}p_{\mu}(t)
                                  = \frac{1}{Z}\sum_{\mu} Q_{\mu}e^{-\beta E_{\mu}} $$
which is the expression for the expectation value of a system in equilibrium.
For example, the internal energy $U$ of a system is given by
$$U = \left \langle E \right \rangle
                                  = \frac{1}{Z}\sum_{\mu} E_{\mu}e^{-\beta E_{\mu}}
                                  = -\frac{1}{Z}\frac{\partial Z}{\partial \beta}
                                  = -\frac{\partial log Z}{\partial \beta}$$
It is sometimes appropriate in MCM calculations to calculate the partition function, from which
many other quantities can be derived. Some examples can be found in Section 1.2 of Newman and
Barkema.

\subsection{Fluctuations, Correlations, and Responses}
It is useful to assess the fluctuations in observable quantities. Doing so
allows us to determine how the quantity we are measuring varies over time and
therefore how much of an approximation is being made by calculating the expectation
value. As an example, the root-mean-square fluctuation in internal energy is given by
$$\sqrt{\left \langle E^{2} \right \rangle - \left \langle E \right \rangle^{2}} = \sqrt{\frac{\partial^{2}log Z}{\partial \beta^{2}}}$$
In the limit of a large system, called the \emph{thermodynamic limit}, it is common for
fluctuations to become negligible. For example, RMS fluctuations in internal energy scale
as $\sqrt{V}$ in a system of volume $V$, but the internal energy itself scales as $V$
so as the system becomes very large, the fluctuations grow increasingly
negligible.

Every parameter that can be fixed in a system (e.g. volume, external field)
has a conjugate variable (e.g. pressure, magnetization) that is given by a
derivative of the free energy. For example,
$$ p = -\frac{\partial F}{\partial V}$$
$$ M = \frac{\partial F}{\partial B}$$
The Hamiltonian contains terms of the form $-XY$ where $Y$ is a "field" whose
value is fixed, and $X$ is the conjugate variable to which it couples.
A common trick to calculate the thermal average of a quantity is the following:
\begin{itemize}
\item Make up a fictitious field that couples to the quantity of interest.
\item Add an appropriate term to the Hamiltonian.
\item Set the field to zero after performing the derivative on the free energy.
\end{itemize}
The \emph{susceptibility} of $X$ to $Y$ is a measurement of the strength of
the response of $X$ to variation in $Y$. Typically, this quantity is denoted
by $\chi$ and can be defined as
$$\chi = \frac{\partial \left \langle X \right \rangle  }{\partial Y}$$
The \emph{linear response theorem} tell us that the fluctuations in a variable
are proportional to the susceptibility of the variable to its conjugate field.

\subsubsection{Correlation Functions}
The \emph{disconnected correlation function} $G^{(2)}(i,j)$ is defined as
$$G^{(2)}(i,j) = \left \langle x_{i}x_{j} \right \rangle$$
This function gives us roughly an idea of how the variables $x_{i}$ and $x_{j}$
are correlated. Consider the following scenarios:
\begin{itemize}
\item If $x_{i}$ and $x_{j}$ move (roughly) together most of the time around
the origin zero, then the average of their product will be a positive value.
\item If $x_{i}$ and $x_{j}$ move (roughly) together but in opposite dirrections
most of the time, then the average of their product will be a negative value.
\item If $x_{i}$ and $x_{j}$ move with no discernible pattern relative to one
another, then their product will be sometimes positive and sometimes negative,
averaging to zero over a long time.
\end{itemize}

This definition can be problematic in the case of a \emph{spontaneously
broken symmetry state}. This occurs when a variable $x$ develops a non-zero
expectation value. If this happens, $x_{i}$ and $x_{j}$ may, for instance,
always be large positive values leading to a large positive correlation value
even if they have nothing to do with one another. To address this problem, we
can use the \emph{two-point connected correlation function} $G_{c}^{(2)}(i,j)$
defined as
$$G_{c}^{(2)}(i,j) = \left \langle x_{i}x_{j} \right \rangle - \left \langle x_{i} \right \rangle \left \langle x_{j} \right \rangle$$
$$ = \left \langle (x_{i} - \left \langle x_{i} \right \rangle) \times (x_{j} - \left \langle x_{j} \right \rangle) \right \rangle $$
For reference, the higher-order connected correlation functions for three and four sites can be found in
section 1.2.1.

\subsection{Example: The Ising Model}
The two-dimensional Ising model of a magnet consists of a lattice of sites, where
each site contains a magnetic dipole or spin. Each spin is a two-level system
denoted by the variable $s_{i}$, which can only take on the values $\pm 1$. Exchange
interactions are modeled by including terms proportional to $s_{i}s_{j}$ in the
Hamiltonian. In the simplest case, these terms are identical for each pair of spins
(indicating the same interaction strength $J$ all over the lattice). Another simplification
is to consider only nearest neighbor interactions. An external magnetic field $B$
can also be introduced. In this case we obtain the following Hamiltonian
$$H = -J\sum_{\left \langle ij \right \rangle }s_{i}s_{j} - B\sum_{i}s_{i}$$
where $\left \langle ij \right \rangle$ indicates a sum over nearest neighbors. The minus
signs are conventional and fix the signs of the parameters $J$ and $B$. In the current form:
\begin{itemize}
\item A positive $J$ value (\emph{ferromagnetic}) makes the spins want to align to minimize the Hamiltonian.
\item A negative $J$ value (\emph{anti-ferromagnetic}) makes the spins want to anti-align.
\item A positive $B$ value makes the spins want to orient positively.
\item A negative $B$ value makes the spins want to orient negatively - that is, they
want to align with the magnetic field.
\end{itemize}
Each state of the Ising system is an specific "setting" of the spins in the lattice. For $N$
sites in the lattice, there are $2^{N}$ states. Earlier we defined the partition function
$Z$ as
$$Z = \sum_{\mu}e^{-\beta E_{\mu}}$$
where \{$\mu$\} represents a set of states. So our Ising partition function is given by
$$Z = \sum_{s_{1} = \pm 1} \sum_{s_{2} = \pm 1}...\sum_{s_{N}=\pm 1}e^{-\beta H} = \sum_{\{s_{i}\}} e^{-\beta H}$$
From the partition function, we can calculate the internal energy, entropy, free energy, specific heat, and so on (see section
1.2). The mean magnetization $\left \langle M \right \rangle $ is given by
$$\left \langle M \right \rangle  = \left \langle \sum_{i}s_{i} \right \rangle $$
and the mean magnetization per spin $\left \langle m \right \rangle$ is just
$$\left \langle m \right \rangle = \frac{1}{N} \left \langle M \right \rangle $$
The magnetic susceptibility is calculated as
$$\frac{\partial \left \langle M \right \rangle }{\partial B} = \beta (\left \langle M^{2} \right \rangle - \left \langle M \right \rangle^{2}  )$$
and the magnetic susceptibility per spin is calculated as
$$\chi = \frac{1}{N}\frac{\partial \left \langle M \right \rangle }{\partial B}$$
The specific heat is given by
$$c = \frac{k\beta^{2}}{N}(\left \langle E^{2} \right \rangle - \left \langle E \right \rangle^{2}  )$$

If we introduce a spatially-varying magnetic field by subscripting $B$, our
Hamiltonian becomes
$$H = -J\sum_{\left \langle ij \right \rangle }s_{i}s_{j} - \sum_{i}B_{i}s_{i}$$
and the mean magnetization per site becomes
$$\left \langle m_{i} \right \rangle = \left \langle s_{i} \right \rangle = \frac{1}{\beta}\frac{\partial \log Z}{\partial B_{i}}$$
which allows us to write down the connected correlation function
$$G_{c}^{(2)}(i,j) = \frac{1}{\beta^{2}}\frac{\partial^{2} \log Z}{\partial B_{i}\partial B_{j}}$$

\subsection{Numerical Methods}
\end{document}
